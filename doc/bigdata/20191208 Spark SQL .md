##  Spark  SQL

> Spark SQL 的前身是 Shark ， Shark 的前身是 Hive  , 在 Shark 中采用 Spark 作为执行引擎， Hive 作为存储 
>
> SparkSQL 是为了摆脱 Hive 的约束，实现 Spark 一栈式服务的目标 

### Spark SQL  是什么

Spark 作为执行引擎， Hive 作为 HIve 存储

Spark SQL 的特点

1. SparkSQL支持查询原生的RDD。 RDD是Spark平台的核心概念，是Spark能够高效的处理大数据的各种场景的基础。		
2. 能够在Scala中写SQL语句。支持简单的SQL语法检查，能够在Scala中写Hive语句访问Hive数据，并将结果取回作为RDD使用。

###  DataFrame

> 与 RDD 类似，DataFrame 是一个分布式数据容器，存储二维结构化数据+数据对应的 schema

<font color='red'>DataFrame就Row类型的DataSet。</font>

###  Spark SQL  DataSource 

SparkSQL的数据源可以是JSON类型的字符串，JDBC,Parquent,Hive，HDFS等。

###  Spark  SQL 底层架构

首先拿到sql后解析一批未被解决的逻辑计划，再经过分析得到分析后的逻辑计划，再经过一批优化规则转换成一批最佳优化的逻辑计划，再经过SparkPlanner的策略转化成一批物理计划，随后经过消费模型转换成一个个的Spark任务执行。

### 谓词下推(Predicate PushDown) 

目的： 提高 Spark SQL 的执行效率，节约时间

![](http://img.zwer.xyz/blog/20191211220433.png)

## 创建 DataFrame 的方式

SparkSQL 既支持原生的  API 查询数据， 也支持通过 SQL  语句的方式查询数据

### 采用 JSON 格式数据作为数据源

  在 Spark SQL  使用 SQL  语句查询时，先要通过`createOrReplaceTempView` 或者 `createGlobalTempView`创建虚拟表，然后调用 SparkSession 中  sql 方法使用。

  注意：

  1. `createOrReplaceTempView` 是不能跨 SparkSession 使用。 `createGlobalTempView` 可以跨 SparkSession 使用，在查询 SQL 语句表名前要加上前缀 `global_temp.`
  2. DataFrame 中 show 方法默认只显示前 20 行数据，使用 shou(行数)显示更多行数

```scala
package com.szxy.myscalacode.sparksql

import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object TestSparkSQLJson {
  def main(args: Array[String]): Unit = {
    val session: SparkSession = SparkSession.builder().master("local").appName("testSql").getOrCreate()
    // 读取 Json 数据两种方式
    // 第一种方式
    // val df: DataFrame = session.read.format("json").load("./data/jsonData")
    // 第二种方式
    val df: DataFrame = session.read.json("./data/jsonfile/jsonData")

    val subSession = session.newSession()
    val nameDF: DataFrame = subSession.read.json("./data/jsonfile/namejson")
    val scoreDF: DataFrame = subSession.read.json("./data/jsonfile/scorejson")
    // createGlobalTempView 可以跨 SparkSession 使用
    // 前提在表名下加上  global_temp.表名
    nameDF.createGlobalTempView("nameTable")
    scoreDF.createGlobalTempView("scoreTable")

    session.sql("select * from global_temp.nameTable").show()
    subSession.sql("select * from global_temp.scoreTable").show()


    // 使用 SQL 语句的形式，就必须要创建虚拟表,
    // 注意：createOrReplaceTempView 不能跨 session 使用
    //df.createOrReplaceTempView("t_json")

    // 使用 SQL 语句
    //session.sql("select name, age from t_json where age > 20").show()

    // select name,age from xxxs;
    // val df2: DataFrame = df.select(df.col("name"), df.col("age"))
    // df2.show()

    // filter 过滤使用  select * from xxx where age = null
    // val filterDF: Dataset[Row] = df.filter("age is null")
    // filterDF.show()

    // select name, age+10 as newAge from xxx
    // val newDF =
    // df.select(df.col("name"), df.col("age").plus(10).alias("newAge")).filter("age > 20 ")
    // newDF.show(3)

  }
}

```


### 读取文件作为 DataFrame 或者 RDD 

  注意要将隐式函数导入

  ```scala
  object CreateDataFrame2 {
    def main(args: Array[String]): Unit = {
      val session: SparkSession = SparkSession.builder().master("local").appName("test").getOrCreate()
  
      import session.implicits._  // 导入隐式函数
  
        /**
         *  读取文件，创建 DateSet 对象
        */
  //    val person: Dataset[String] = session.read.textFile("./data/jsonfile/People.txt")
  //
  //    import session.implicits._  // 导入隐式函数
  //    val personDS: Dataset[Person] = Person.map(line => {
  //      val arr: Array[String] = line.split(",")
  //      Person(arr(0).toInt, arr(1), arr(2).toInt, arr(3).toInt)
  //    })
  
      /**
       *  读取文件，创建 DateSet 对象
       */
      val rdd: RDD[String] = session.sparkContext.textFile("./data/jsonfile/People.txt")
      val PersonDS: RDD[Person] = rdd.map(line => {
        val arr: Array[String] = line.split(",")
        Person(arr(0).toInt, arr(1), arr(2).toInt, arr(3).toInt)
      })
  
      val frame: DataFrame = PersonDS.toDF()
  //    frame.show()
  
      /**
       *   DataFram  API  操作
       * */
      frame.createOrReplaceTempView("t_people")
      val df: DataFrame = session.sql("select name,age,score from t_people where age between 19 and 25 ")
  //    df.show()
      // 第一种方式
      df.map((teenager : Row) => "name： " + teenager(0)).show()
      // 第二种方式， 推荐第二种
      df.map((teenager : Row) => "name： " + teenager.getAs[String]("name")).show()
  
  
    }
  }
  ```

### 根据 schema 文件创建 DataFrame 

  ```scala
  package com.szxy.myscalacode.sparksql
  
  
  import org.apache.spark.rdd.RDD
  import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}
  import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
  
  object CreateDataFrameFromSchema {
    def main(args: Array[String]): Unit = {
      val session: SparkSession = SparkSession.builder().master("local").appName("createDataFrameFromSchema").getOrCreate()
      // 导入隐式函数
      import session.implicits._
      val persons: RDD[String] = session.sparkContext.textFile("./data/jsonfile/people.txt")
  
      /**
       *  方式一
       *  */
  //    val peopleRDD: RDD[Row] = persons.map(line => {
  //      val arr: Array[String] = line.split(",")
  //      Row(arr(0).toInt, arr(1), arr(2).toInt, arr(3).toInt)
  //    });
  //
  //    // 创建约束文件
  //    val peopleSchema: StructType = StructType(List[StructField](
  //      StructField("id", IntegerType, true),
  //      StructField("name", StringType, true),
  //      StructField("age", IntegerType, true),
  //      StructField("score", IntegerType, true)
  //    ))
  
      /**
       *  方式二 ： 动态创建 Schema  
       */
      var schemaString  ="id name age score"
      val fields: Array[StructField] = schemaString.split(" ").map(filedName => {
        StructField(filedName, StringType, true)
      })
      val peopleSchema: StructType = StructType(fields)
  
      val peopleRDD: RDD[Row] = persons.map(line => {
        val arr: Array[String] = line.split(",")
        Row(arr(0), arr(1), arr(2), arr(3))
      })
      
  
      // 创建 DataFrame
      val df: DataFrame = session.createDataFrame(peopleRDD, peopleSchema)
      df.show()
      df.printSchema()
      // 创建虚拟表
      df.createOrReplaceTempView("people")
  
      
      val result: DataFrame = session.sql("select name, age, score from people")
      result.map(row => {"name : " + row(0)}).as("myColumn").show()
  
  
    }
  }
  
  ```


### 读取和保存  parquet  文件

  ```scala
  package com.szxy.myscalacode.sparksql
  
  import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
  
  object CreateDataFrameFromParquet {
    def main(args: Array[String]): Unit = {
      val session: SparkSession = SparkSession.builder().master("local").appName("createDataFrameFromParquet").getOrCreate()
  
      val df: DataFrame = session.read.text("./data/jsonfile/jsonData")
      df.show()
  
      /**
       * 保存成parquet文件
       *  Append: 如果文件存在就追加
       *  Overwrite:覆盖写
       *  Ignore:忽略
       *  ErrorIfExists：报错
       */
      df.write.mode(SaveMode.Append).format("parquet").save("./data/parquet")
  
  
      /**
       *  读取 parquet 文件
       */
      val df2: DataFrame = session.read.parquet("./data/parquet")
      val count: Long = df2.count()
      println("count ="+ count)
      df2.show(100)
    }
  }
  
  
  ```

### 读取 MySQL 和 存储数据在 MySQL

  ```scala
  object CreateDataFrameFromJDBC {
    def main(args: Array[String]): Unit = {
      val session: SparkSession =
        SparkSession.builder()
          .master("local")
          .appName("createDataFrameFromJDBC")
          .config("spark.sql.shuffle.partitions", 1)  
          .getOrCreate()
  
      /**
       *  读取 MySQL 数据库第一种方式
       */
  //    val properties = new Properties()
  //    properties.setProperty("user", "root")
  //    properties.setProperty("password", "root")
  //    val df: DataFrame = 
        session.read.jdbc("jdbc:mysql://127.0.0.1:3306/test", table = "person", properties);
  //    df.show(100)
  
      /**
       *  读取 MySQL  数据库第二种方式
       */
      val map = Map[String, String](
        "driver" -> "com.mysql.jdbc.Driver",
        "url" -> "jdbc:mysql://localhost:3306/test",
        "user" -> "root",
        "password" -> "root",
        "dbtable"->"score"//表名
      )
  
      val df2: DataFrame = session.read.format("jdbc").options(map).load()
      //df2.show()
  
      /**
       *  读取 MySQL  数据第三种方式
       */
      val df3: DataFrame = session.read.format("jdbc")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("url", "jdbc:mysql://localhost:3306/test")
        .option("user", "root")
        .option("password", "root")
        .option("dbtable", "person")
        .load()
  
      // df3.show()
  
      /**
       *   保存数据到 MySQL 数据库中
       */
      df2.createOrReplaceTempView("score")
      df3.createOrReplaceTempView("person")
  
      val result: DataFrame =
        session.sql("select p.id,p.name,p.age,s.score from person p join score s on p.id = s.id")
      result.show()
      val properties = new Properties()
      properties.setProperty("user", "root")
      properties.setProperty("password", "root")
      // 入库
      result.write.mode(SaveMode.Append).jdbc("jdbc:mysql://localhost:3306/test", "result", properties)
  
    }
  }
  
  ```

## Spark SQL 操作 Hive

  ```
  package com.szxy.myscalacode.sparksql
  
  import org.apache.spark.sql.{SaveMode, SparkSession}
  
  /**
    * 读取Hive中的数据
    * 要开启 ：enableHiveSupport
    */
  object CreateDataFrameFromHive {
    def main(args: Array[String]): Unit = {
      val spark = SparkSession.builder().appName("CreateDataFrameFromHive").enableHiveSupport().getOrCreate()
      spark.sql("use spark")
      spark.sql("drop table if exists student_infos")
      spark.sql("create table if not exists student_infos (name string,age int) row format  delimited fields terminated by '\t'")
      spark.sql("load data local inpath '/root/test/student_infos' into table student_infos")
  
      spark.sql("drop table if exists student_scores")
      spark.sql("create table if not exists student_scores (name string,score int) row format delimited fields terminated by '\t'")
      spark.sql("load data local inpath '/root/test/student_scores' into table student_scores")
  //    val frame: DataFrame = spark.table("student_infos")
  //    frame.show(100)
  
      val df = spark.sql("select si.name,si.age,ss.score from student_infos si,student_scores ss where si.name = ss.name")
      df.show(100)
      spark.sql("drop table if exists good_student_infos")
      /**
        * 将结果写入到hive表中
        */
      df.write.mode(SaveMode.Overwrite).saveAsTable("good_student_infos")
  
    }
  }
  
  
  ```



## UDF 用户自定义函数

**UDF 函数使用步骤**

1. 注册一个 UDF 函数

   这里以计算字符串长度的 UDF 函数举例 

   ```scala
   session.udf.register("strLength" , (name : String) =>{
         name.length()
   })
   ```
   
2. 在 SQL 语句中使用 UDF 函数

   ```scala
   val resultDF: DataFrame = 
   session.sql("select name, strLength(name) as nameLength from person")
   
   ```

**实现代码如下所示**

```scala
/**
 *  UDF  用户自定义函数
 */
object UDFDemo {
  def main(args: Array[String]): Unit = {
      val session: SparkSession = SparkSession.builder().master("local").appName("udfDemo").getOrCreate()
      // 读取从数组读取数据
      val persons: RDD[String] = session.sparkContext.parallelize(Array[String]("zhangsan", "lisi", "wangwu", "zhaoliu", "tianqi "))
      // 导入隐式函数
      import session.implicits._
      // 将 RDD 转为 DataFrame
      val personDF: DataFrame = persons.toDF("name")
      // personDF.show()
    /**
     *   注册 UDF 用户自定义函数
     *   功能是计算 name 的长度
     */
    session.udf.register("strLength" , (name : String) =>{
         name.length()
      })
      // 创建视图表
      personDF.createOrReplaceTempView("person")
      val resultDF: DataFrame = session.sql("select name, strLength(name) as nameLength from person")
      resultDF.show()
  }
}
```


##  UDAF 用户自定义聚合函数

**UDAF 函数使用步骤**

1. 创建一个类继承 UserDefinedAggregateFunction
2. 重写  UserDefinedAggregateFunction 中方法（注意：initialize、update、merge 方法的原理和使用）
3. 注册用户自定义聚合函数
4. 在 SQL 语句中使用 UDAF 函数

**图解 UDAF 主要运行过程，这里以聚合函数 count的功能 为例，统计每个名字出现的次数**

![](http://img.zwer.xyz/blog/UDAF.jpg)



**代码具体实现如下**

```scala
package com.szxy.myscalacode.udaf

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DataTypes, IntegerType, StringType, StructField, StructType}

/**
 *  用户自定义聚合函数
 */
class MyUdaf extends UserDefinedAggregateFunction{
    // 输入数据的类型
    override def inputSchema: StructType = {
    DataTypes.createStructType(Array[StructField](StructField("xxx",StringType, true)))
    }
    // 聚合操作过程中， 所处理的数据类型
    override def bufferSchema: StructType = {
    DataTypes.createStructType(Array[StructField](StructField("jjj",IntegerType, true)))
    }
    // 返回值类型
    override def dataType: DataType = {
    IntegerType
    }
    // 执行多次， 一致性相同
    override def deterministic: Boolean = {
    true
    }
    // 初始化
    override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0
    }
    // 更新， 这相当于 task 任务中 map 阶段
    override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    buffer(0) = buffer.getAs[Int](0) + 1
    }
    // 合并， 这相当于 task 任务中 reduce 阶段
    override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getAs[Int](0) + buffer2.getAs[Int](0)
    }
    // 最后返回一个最终的聚合值要和 dataType 的类型一一对应
    override def evaluate(buffer: Row): Any = {
      buffer.getAs[Int](0)
    }
}

object UDAFDemo {
  def main(args: Array[String]): Unit = {
    val session: SparkSession = SparkSession.builder().master("local").appName("xxx").getOrCreate()

    val rdd: RDD[String] = session.sparkContext.parallelize(Array[String]("zhangsan", "lisi", "lisi",
      "zhangsan", "zhangsan", "zhangsan", "wangwu", "wangwu", "wangwu"))
    
    // 导入隐式函数
    import session.implicits._
    
    val personDF = rdd.toDF("name")
    personDF.show()
    personDF.createOrReplaceTempView("person")
    /**
     *   注册 UDAF 函数
     */
    session.udf.register("mycount", new MyUdaf)
    
    // select name, count(name) from person group by name
    val result: DataFrame = session.sql("select name, mycount(name) as nameNum from person group by name")
    result.show()
  }
}
```

## 开窗函数

### 开窗函数之 over 函数

over函数的作用： 按照某个字段排序并分组

注意: 在 Hive 中支持 over 函数，Oracle 也支持 over 函数， 但是*在 MySQL 5.0+ 中不支持 over 函数*，MySQL 8.0+ 支持 over 函数

小技巧： Linux 中 Ctrl +R 键搜索历史命令

```scala
package com.bjsxt.scalaspark.sql.windows


import org.apache.spark.sql.{SaveMode, SparkSession}

/**
  * over 窗口函数
  * row_number() over(partition by xx order by xx) as rank
  * rank 在每个分组内从1开始
  */
object OverFunctionOnHive {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("over").enableHiveSupport().getOrCreate()
    spark.sql("use spark")
    spark.sql("create table if not exists sales (riqi string,leibie string,jine Int) " + "row format delimited fields terminated by '\t'")
    spark.sql("load data local inpath '/root/test/sales' into table sales")

    /**
      * rank 在每个组内从1开始
      *   5 A 200   --- 1
      *   3 A 100   ---2
      *   4 A 80   ---3
      *   7 A 60   ---4
      *
      *   1 B 100   ---1
      *   8 B 90  ---2
      *   6 B 80  ---3
      *   1 B 70  ---4
      */
    val result = spark.sql(
      "select"
                +" riqi,leibie,jine "
              + "from ("
                  + "select "
                      +"riqi,leibie,jine,row_number() over (partition by leibie order by jine desc) rank "
                  + "from sales) t "
              + "where t.rank<=3")
    result.write.mode(SaveMode.Append).saveAsTable("salesResult")
    result.show(100)
  }
}

```















