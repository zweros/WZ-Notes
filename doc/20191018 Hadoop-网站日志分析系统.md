---
title: 20191018 Hadoop 网站日志分析系统
date: 2019-10-08
---

## 需求



### 数据流图

![](http://img.zwer.xyz/blog/20191018204503.png)



### JS-SDK 





### Java-SDK 

BlockingQueue 使用 ：

https://app.yinxiang.com/shard/s66/nl/15607671/d69a1aac-5cd6-46fd-ad1f-b28a4d7850b4

## Nginx 搭建

### 注册 Nginx 服务

```shell
安装之前准备
1、依赖 gcc openssl-devel pcre-devel zlib-devel
	安装：yum install gcc openssl-devel pcre-devel zlib-devel -y

安装Nginx
./configure 

make && make install

默认安装目录：
/usr/local/nginx


配置Nginx为系统服务，以方便管理
  1、在/etc/rc.d/init.d/目录中建立文本文件nginx
  2、在文件中粘贴下面的内容：
#!/bin/sh
#
# nginx - this script starts and stops the nginx daemon
#
# chkconfig:   - 85 15 
# description:  Nginx is an HTTP(S) server, HTTP(S) reverse \
#               proxy and IMAP/POP3 proxy server
# processname: nginx
# config:      /etc/nginx/nginx.conf
# config:      /etc/sysconfig/nginx
# pidfile:     /var/run/nginx.pid
 
# Source function library.
. /etc/rc.d/init.d/functions
 
# Source networking configuration.
. /etc/sysconfig/network
 
# Check that networking is up.
[ "$NETWORKING" = "no" ] && exit 0
 
nginx="/usr/local/nginx/sbin/nginx"
prog=$(basename $nginx)
 
NGINX_CONF_FILE="/usr/local/nginx/conf/nginx.conf"
 
[ -f /etc/sysconfig/nginx ] && . /etc/sysconfig/nginx
 
lockfile=/var/lock/subsys/nginx
 
make_dirs() {
   # make required directories
   user=`nginx -V 2>&1 | grep "configure arguments:" | sed 's/[^*]*--user=\([^ ]*\).*/\1/g' -`
   options=`$nginx -V 2>&1 | grep 'configure arguments:'`
   for opt in $options; do
       if [ `echo $opt | grep '.*-temp-path'` ]; then
           value=`echo $opt | cut -d "=" -f 2`
           if [ ! -d "$value" ]; then
               # echo "creating" $value
               mkdir -p $value && chown -R $user $value
           fi
       fi
   done
}
 
start() {
    [ -x $nginx ] || exit 5
    [ -f $NGINX_CONF_FILE ] || exit 6
    make_dirs
    echo -n $"Starting $prog: "
    daemon $nginx -c $NGINX_CONF_FILE
    retval=$?
    echo
    [ $retval -eq 0 ] && touch $lockfile
    return $retval
}
 
stop() {
    echo -n $"Stopping $prog: "
    killproc $prog -QUIT
    retval=$?
    echo
    [ $retval -eq 0 ] && rm -f $lockfile
    return $retval
}
 
restart() {
    configtest || return $?
    stop
    sleep 1
    start
}
 
reload() {
    configtest || return $?
    echo -n $"Reloading $prog: "
    killproc $nginx -HUP
    RETVAL=$?
    echo
}
 
force_reload() {
    restart
}
 
configtest() {
  $nginx -t -c $NGINX_CONF_FILE
}
 
rh_status() {
    status $prog
}
 
rh_status_q() {
    rh_status >/dev/null 2>&1
}
 
case "$1" in
    start)
        rh_status_q && exit 0
        $1
        ;;
    stop)
        rh_status_q || exit 0
        $1
        ;;
    restart|configtest)
        $1
        ;;
    reload)
        rh_status_q || exit 7
        $1
        ;;
    force-reload)
        force_reload
        ;;
    status)
        rh_status
        ;;
    condrestart|try-restart)
        rh_status_q || exit 0
            ;;
    *)
        echo $"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}"
        exit 2
esac

3、修改nginx文件的执行权限
	chmod +x nginx
4、添加该文件到系统服务中去
	chkconfig --add nginx
	查看是否添加成功
	chkconfig --list nginx

启动，停止，重新装载
service nginx start|stop
```

### Nginx.conf   配置

```shell
#user  nobody;
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;
    
    log_format my_format '$remote_addr^A$msec^A$http_host^A$request_uri';
    
    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    server {
        listen       80;
        server_name  localhost;

        location / {
            root   html;
            index  index.html index.htm;
        }
        location = /log.gif {
            default_type image/gif;
            access_log /opt/data/access.log my_format;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
      
    }
}
```

## Flume

### Flume 简介

> Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.

It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.

![](http://flume.apache.org/_images/DevGuide_image00.png)



Flume 官网：http://flume.apache.org/

### Flume 安装及配置

```shell
安装
1、上传
2、解压
3、修改conf/flume-env.sh  文件中的JDK目录
 注意：JAVA_OPTS 配置  如果我们传输文件过大 报内存溢出时 需要修改这个配置项
4、验证安装是否成功  ./flume-ng version
5、配置环境变量
	export FLUME_HOME=/home/apache-flume-1.6.0-bin


Source、Channel、Sink有哪些类型
    Flume Source
	Source类型 	              | 说明
	Avro Source 	            | 支持Avro协议（实际上是Avro RPC），内置支持
	Thrift Source 	          | 支持Thrift协议，内置支持
	Exec Source 	            | 基于Unix的command在标准输出上生产数据
	JMS Source 	              | 从JMS系统（消息、主题）中读取数据
	Spooling Directory Source | 监控指定目录内数据变更
	Twitter 1% firehose Source|	通过API持续下载Twitter数据，试验性质
	Netcat Source 	          | 监控某个端口，将流经端口的每一个文本行数据作为Event输入
	Sequence Generator Source | 序列生成器数据源，生产序列数据
	Syslog Sources 	          | 读取syslog数据，产生Event，支持UDP和TCP两种协议
	HTTP Source 	            | 基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式
	Legacy Sources 	          | 兼容老的Flume OG中Source（0.9.x版本）

    Flume Channel
	Channel类型 	  说明
	Memory Channel 	           | Event数据存储在内存中
	JDBC Channel   	           | Event数据存储在持久化存储中，当前Flume Channel内置支持Derby
	File Channel   	           | Event数据存储在磁盘文件中
	Spillable Memory Channel   | Event数据存储在内存中和磁盘上，当内存队列满了，会持久化到磁盘文件
	Pseudo Transaction Channel | 测试用途
	Custom Channel 	           | 自定义Channel实现

    Flume Sink
	Sink类型 	说明
	HDFS Sink 	        | 数据写入HDFS
	Logger Sink 	      | 数据写入日志文件
	Avro Sink 	        | 数据被转换成Avro Event，然后发送到配置的RPC端口上
	Thrift Sink 	      | 数据被转换成Thrift Event，然后发送到配置的RPC端口上
	IRC Sink    	      | 数据在IRC上进行回放
	File Roll Sink 	    | 存储数据到本地文件系统
	Null Sink 	        | 丢弃到所有数据
	HBase Sink 	        | 数据写入HBase数据库
	Morphline Solr Sink | 数据发送到Solr搜索服务器（集群）
	ElasticSearch Sink 	| 数据发送到Elastic Search搜索服务器（集群）
	Kite Dataset Sink 	| 写数据到Kite Dataset，试验性质的
	Custom Sink 	      | 自定义Sink实现



案例1、 A simple example
	http://flume.apache.org/FlumeUserGuide.html#a-simple-example
	
	配置文件
	############################################################
	# Name the components on this agent
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = netcat
	a1.sources.r1.bind = localhost // 改成主机名
	a1.sources.r1.port = 44444

	# Describe the sink
	a1.sinks.k1.type = logger

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################

启动flume
flume-ng agent -n a1 -c conf -f simple.conf -Dflume.root.logger=INFO,console

安装telnet
yum install telnet
退出 ctrl+]  quit

Memory Chanel 配置
  capacity：默认该通道中最大的可以存储的event数量是100，
  trasactionCapacity：每次最大可以source中拿到或者送到sink中的event数量也是100
  keep-alive：event添加到通道中或者移出的允许时间
  byte**：即event的字节量的限制，只包括eventbody



案例2、两个flume做集群

	node01服务器中，配置文件
	############################################################
	# Name the components on this agent
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = netcat
	a1.sources.r1.bind = node1
	a1.sources.r1.port = 44444

	# Describe the sink
	# a1.sinks.k1.type = logger
	a1.sinks.k1.type = avro
	a1.sinks.k1.hostname = node2
	a1.sinks.k1.port = 60000

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################
	
	node02服务器中，安装Flume（步骤略）
	配置文件
	############################################################
	# Name the components on this agent
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = avro
	a1.sources.r1.bind = node2
	a1.sources.r1.port = 60000

	# Describe the sink
	a1.sinks.k1.type = logger

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################
	
	先启动node02的Flume
	flume-ng agent  -n a1 -c conf -f avro.conf -Dflume.root.logger=INFO,console
	
	再启动node01的Flume
	flume-ng agent  -n a1 -c conf -f simple.conf2 -Dflume.root.logger=INFO,console
	
	打开telnet 测试  node02控制台输出结果


案例3、Exec Source
		http://flume.apache.org/FlumeUserGuide.html#exec-source
		
	配置文件
	############################################################
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = exec
	a1.sources.r1.command = tail -F /home/flume.exec.log

	# Describe the sink
	a1.sinks.k1.type = logger
	
	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################
	
	启动Flume
	flume-ng agent -n a1 -c conf -f exec.conf -Dflume.root.logger=INFO,console
	
	创建空文件演示 touch flume.exec.log
	循环添加数据
	for i in {1..50}; do echo "$i hi flume" >> flume.exec.log ; sleep 0.1; done
		
案例4、Spooling Directory Source
		http://flume.apache.org/FlumeUserGuide.html#spooling-directory-source
	配置文件
	############################################################
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = spooldir
	a1.sources.r1.spoolDir = /home/logs
	a1.sources.r1.fileHeader = true

	# Describe the sink
	a1.sinks.k1.type = logger

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################

	启动Flume
	flume-ng agent -n a1 -c conf -f spool.conf -Dflume.root.logger=INFO,console

	拷贝文件演示
	mkdir logs
	cp flume.exec.log logs/


案例5、hdfs sink
		http://flume.apache.org/FlumeUserGuide.html#hdfs-sink
	
		配置文件
	############################################################
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Describe/configure the source
	a1.sources.r1.type = spooldir
	a1.sources.r1.spoolDir = /home/logs
	a1.sources.r1.fileHeader = true

	# Describe the sink
	***只修改上一个spool sink的配置代码块 a1.sinks.k1.type = logger
	a1.sinks.k1.type=hdfs
	a1.sinks.k1.hdfs.path=hdfs://bjsxt/flume/%Y-%m-%d/%H%M
	
	##每隔60s或者文件大小超过10M的时候产生新文件
	# hdfs有多少条消息时新建文件，0不基于消息个数
	a1.sinks.k1.hdfs.rollCount=0
	# hdfs创建多长时间新建文件，0不基于时间,时间单位 s
	a1.sinks.k1.hdfs.rollInterval=60
	# hdfs多大时新建文件，0不基于文件大小
	a1.sinks.k1.hdfs.rollSize=10240
	# 当目前被打开的临时文件在该参数指定的时间（秒）内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件
	a1.sinks.k1.hdfs.idleTimeout=3
	
	a1.sinks.k1.hdfs.fileType=DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp=true
	
	## 每五分钟生成一个目录:
	# 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”，后面再介绍。如果启用，则会影响除了%t的其他所有时间表达式
	a1.sinks.k1.hdfs.round=true
	# 时间上进行“舍弃”的值；
	a1.sinks.k1.hdfs.roundValue=5
	# 时间上进行”舍弃”的单位，包含：second,minute,hour
	a1.sinks.k1.hdfs.roundUnit=minute

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
	############################################################
	创建HDFS目录
	hadoop fs -mkdir /flume
	
	启动Flume
	flume-ng agent -n a1 -c conf -f hdfs.conf -Dflume.root.logger=INFO,console

	查看hdfs文件
	hadoop fs -ls /flume/...
	hadoop fs -get /flume/...

作业：
1、flume如何收集java请求数据
2、项目当中如何来做？ 日志存放/log/目录下 以yyyyMMdd为子目录 分别存放每天的数据

```

