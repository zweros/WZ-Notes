---

stitle: 20191225 Flink
date: 2019-12-25
---
#  Flink
[TOC]



## Flink 概念

> **Apache Flink® — Stateful Computations over Data Streams**
>
> 翻译过来的意思是基于数据流的有状态计算

 ![](https://flink.apache.org/img/flink-home-graphic.png)

## Storm & SparkStreaming  & Flink 对比

|                | 处理数据的方式         | 延时度 | 吞吐量 |
| -------------- | ---------------------- | ------ | ------ |
| Storm          | 实时处理               | 低     | 低     |
| SparkStreaming | 微批流式处理           | 高     | 高     |
| Flink          | 支持实时处理的流式处理 | 低     | 高     |

![](http://img.zwer.xyz/blog/MR、Spark、Storm、Flink.jpg)

## Flink 版本 WordCount

### 搭建 Flink 的Maven 环境

- pom.xml 

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  
  <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
  
    <groupId>com.szxy</groupId>
    <artifactId>flik-workcount-demo</artifactId>
    <version>1.0-SNAPSHOT</version>
  
    <name>flik-workcount-demo</name>
    <!-- FIXME change it to the project's website -->
    <url>http://www.example.com</url>
  
    <properties>
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
      <maven.compiler.source>1.8</maven.compiler.source>
      <maven.compiler.target>1.8</maven.compiler.target>
      <flink.version>1.7.1</flink.version>
    </properties>
  
    <dependencies>
      <!-- Flink 依赖 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-java</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-wikiedits_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <!-- Flink Kafka连接器的依赖 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka-0.11_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.11</version>
        <scope>test</scope>
      </dependency>
  
      <!-- Flink Scala2.11 版本 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-scala_2.11</artifactId>
        <version>1.7.1</version>
      </dependency>
  
      <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>2.11.12</version>
      </dependency>
  
     <!-- log4j 和slf4j 包,如果在控制台不想看到日志，可以将下面的包注释掉-->
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.25</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>1.2.17</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>1.7.25</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-nop</artifactId>
        <version>1.7.25</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>1.7.5</version>
      </dependency>
    </dependencies>
  
    <build>
      <plugins>
        <!-- 在maven项目中既有java又有scala代码时配置 maven-scala-plugin 插件打包时可以将两类代码一起打包 -->
        <plugin>
          <groupId>org.scala-tools</groupId>
          <artifactId>maven-scala-plugin</artifactId>
          <version>2.15.2</version>
          <executions>
            <execution>
              <goals>
                <goal>compile</goal>
                <goal>testCompile</goal>
              </goals>
            </execution>
          </executions>
        </plugin>
        <plugin>
          <artifactId>maven-assembly-plugin</artifactId>
          <version>2.4</version>
          <configuration>
            <!-- 设置false后是去掉 MySpark-1.0-SNAPSHOT-jar-with-dependencies.jar 后的 “-jar-with-dependencies” -->
            <!--<appendAssemblyId>false</appendAssemblyId>-->
            <descriptorRefs>
              <descriptorRef>jar-with-dependencies</descriptorRef>
            </descriptorRefs>
            <archive>
              <manifest>
                <mainClass>com.lw.myflink.Streaming.FlinkSocketWordCount</mainClass>
              </manifest>
            </archive>
          </configuration>
          <executions>
            <execution>
              <id>make-assembly</id>
              <phase>package</phase>
              <goals>
                <goal>assembly</goal>
              </goals>
            </execution>
          </executions>
        </plugin>
      </plugins>
    </build>
  </project>
  ```

### 编写单词统计  Java 代码

- 搭建 Flink 批处理环境

  ```java
  // 创建 Flink 流式处理执行环境
  //StreamExecutionEnvironment execStreamEvn = 		StreamExecutionEnvironment.getExecutionEnvironment();
  ```

- 搭建 Flink 流式处理环境

  ```java
  // 创建 Flink 批处理执行环境
  ExecutionEnvironment execEvn = ExecutionEnvironment.getExecutionEnvironment();
  ```

- FlinkWordCount.java

  ```java
  package com.szxy;
  
  import org.apache.flink.api.common.functions.FlatMapFunction;
  import org.apache.flink.api.common.functions.MapFunction;
  import org.apache.flink.api.java.ExecutionEnvironment;
  import org.apache.flink.api.java.operators.*;
  import org.apache.flink.api.java.tuple.Tuple2;
  import org.apache.flink.util.Collector;
  
  /**
   * @Auther:zwer
   * @Date:2019/12/25 15:23
   * @Description:com.szxy
   * @Version:1.0
   * Flink WordCount 单词个数统计
   **/
  public class FlikWordCountDemo {
  
      public static void main(String[] args) throws Exception {
          // 创建 Flink 执行环境
          ExecutionEnvironment execEvn = ExecutionEnvironment.getExecutionEnvironment();
          DataSource<String> dataSource = execEvn.readTextFile("F:\\workspacesIDEA\\flik-workcount-demo\\data\\words.txt");
          // 拆分单词    "hello World" => "hello","word"
          FlatMapOperator<String, String> words = dataSource.flatMap(new FlatMapFunction<String, String>() {
              @Override
              public void flatMap(String line, Collector<String> collector) throws Exception {
                  String[] split = line.split(" ");
                  for (String s : split) {
                      collector.collect(s);
                  }
              }
          });
          // 为单词，标注数字  "word"=>"(word,1)"
          // 注意：这里的 Tuple2<T,T> 使用是 Flink 包下的
          MapOperator<String, Tuple2<String, Integer>> pairWord = words.map(new MapFunction<String, Tuple2<String, Integer>>() {
              @Override
              public Tuple2<String, Integer> map(String word) throws Exception {
                  return new Tuple2(word, 1);
              }
          });
          // 统计每个单词出现的次数
          UnsortedGrouping<Tuple2<String, Integer>> groupBy = pairWord.groupBy(0);
          AggregateOperator<Tuple2<String, Integer>> result = groupBy.sum(1);
          result.print();
  
      }
  
  }
  
  ```

### Flink 处理数据流程 & 代码执行流程

- Flink处理数据流程
  	Source -> Transformations  -> Sink
    	数据源头  ->  数据转换  -> 数据输出 

- Flink程序的执行过程：

  ```
  获取flink的执行环境(execution environment)
  加载数据-- soure
  对加载的数据进行转换 -- transformation
  对结果进行保存或者打印 --sink
  触发flink程序的执行(execute(),count(),collect(),print())，例如：调用ExecutionEnvironment或者StreamExecutionEnvironment的execute()方法。
  ```

### 注意事项

1. `Flink` 不是基于K,V 格式处理数据，但可以指定某些虚拟 Key 
2. `Flink` 中批处理的数据对象是 `DataSet`, 而流式处理的数据对象是 `DataStream`
3. `Flink` 的代码流程必须符合 `Source -> Transformaction -> sink` 。transformation 都是懒执行，需要最后使用 `env.execute() `触发执行或者会用` print()`、`count()`、`collect()`触发执行

4. Flink 中 Tuple 支持25个元素，下标从 0 开始

## Flink 中全局排序 & 分区排序

在客户端 Eclipse 测试中 Flink 默认使用 8 个分区

- 设置 Flink 并行度

```java
//  设置并行度，默认的并行度为 8
execEvn.setParallelism(1);
```

- 排序

  ```java
  // 统计每个单词出现的次数
  UnsortedGrouping<Tuple2<String, Integer>> groupBy = pairWord.groupBy(0);
  AggregateOperator<Tuple2<String, Integer>> result = groupBy.sum(1);
  // 分区内排序
  SortPartitionOperator<Tuple2<String, Integer>> sortResult =
      result.sortPartition(1, Order.DESCENDING);
  sortResult.print();
  ```

当 Flink 并行度为  1 时，分区内排序恰好时全局排序

## Flink  Source & Sink

### Source  

```

```

### Sink

```

```

## Flink 累加器  & 计数器

累加器与计数器之间的关系是**计算器是累加器的具体实现**

计数器有：` IntCounter` 、`DoubleCounter`、`LongCounter ` 

```java
/**
 * @Auther:zwer
 * @Date:2019/12/25 17:28
 * @Description:com.szxy
 * @Version:1.0
 * Flink 中累加器的使用， 累加器在 Flink 中具体表现为计数器
 * 需求： 读取文本文件，统计其中以 https:// 开头的网址个数（默认一个网址占据一行）
 **/
public class FlinkAccumulatorDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment execEnv = ExecutionEnvironment.getExecutionEnvironment();
        execEnv.setParallelism(1);
        DataSource<String> dataSource = execEnv.readTextFile("./data/website.txt");
        // 过滤以 https:// 开头的数据
        FilterOperator<String> filterSites = dataSource.filter(new FilterFunction<String>() {
            @Override
            public boolean filter(String s) throws Exception {
                return s.startsWith("https://");
            }
        });
        /**
         *  若要使用累加器，需要使用  RichMapFunction<String,T>
         */
        MapOperator<String, String> mapSites = filterSites.map(new RichMapFunction<String, String>() {
            /**
             *   int 类型的计算器
             */
            private IntCounter counter = new IntCounter();

            /**
             * 注册累加器
             * @param parameters
             * @throws Exception
             */
            @Override
            public void open(Configuration parameters) throws Exception {
                // 注册累加器  第一个参数表示累计器的名称，第二个参数表示累计器具体实现类型
                getRuntimeContext().addAccumulator("myAcc", counter);
            }

            /**
             * 计算
             * @param s  一行数据
             * @return s  一行数据
             * @throws Exception
             */
            @Override
            public String map(String s) throws Exception {
                counter.add(1); // 每遍历一个数据，统计的个数加 1
                return s;
            }
        });

        DataSink<String> dataSink =
                mapSites.writeAsText("./data/result/websitesCountResult.txt", FileSystem.WriteMode.OVERWRITE);
        JobExecutionResult webSiteJob = execEnv.execute("countWebSiteJob");
        int myAcc = webSiteJob.getAccumulatorResult("myAcc");
        System.out.println("myAcc of ressult = "+myAcc);
    }
}
```

## Flink 编程模型

- Flink中数据类型
  有界数据流  
  无界数据流

- Flink三种处理数据模型
   Flink批处理
  Flink批处理中处理的是有界数据流   --Dataset

- Flink流式处理
  Flink流式处理中有界数据流也有无界数据流  --DataStream

- FlinkSQL处理
  有界数据流也有无界数据流

## Apache Flink 术语

### Dataflows

![](http://img.zwer.xyz/blog/20191225192404.png)

### parallel Dataflow

![](http://img.zwer.xyz/blog/20191225192433.png)

### Task 和 算子链

![](http://img.zwer.xyz/blog/20191225192509.png)

### JobManager、TaskManager和clients:

Flink运行时包含两种类型的进程：

- JobManger:也叫作masters,协调分布式执行，调度task,协调checkpoint，协调故障恢复。在Flink程序中至少有一个JobManager,高可用可以设置多个JobManager,其中一个是Leader,其他都是standby状态。

- TaskManager:也叫workers,执行dataflow生成的task,负责缓冲数据，及TaskManager之间的交换数据。Flink程序中必须有一个TaskManager。

  Flink程序可以运行在standalone集群，Yarn或者Mesos资源调度框架中。

- clients不是Flink程序运行时的一部分，作用是向JobManager准备和发送dataflow,之后，客户端可以断开连接或者保持连接。

![](http://img.zwer.xyz/blog/20191225192539.png)

### TaskSlot 任务槽

每个Worker（TaskManager）是一个JVM进程，可以执行一个或者多个task,这些task可以运行在任务槽上，每个worker上至少有一个任务槽。每个任务槽都有固定的资源，例如：TaskManager有三个TaskSlots,那么每个TaskSlot会将TaskMananger中的内存均分，即每个任务槽的内存是总内存的1/3。**任务槽的作用就是分离任务的托管内存，不会发生cpu隔离。**
通过调整任务槽的数据量，用户可以指定每个TaskManager有多少任务槽，更多的任务槽意味着更多的task可以共享同一个JVM,同一个JVM中的task共享TCP连接和心跳信息，共享数据集和数据结构，从而减少TaskManager中的task开销。

总结：task slot的个数代表TaskManager可以并行执行的task数。

![](http://img.zwer.xyz/blog/20191225193421.png)



## Flink 集群搭建

### 节点分布

|             | node01 | node02 | node03 |
| ----------- | ------ | ------ | ------ |
| JobManager  | *      |        |        |
| TaskManager |        | *      | *      |

### Flink 集群搭建步骤

```shell
进入https://flink.apache.org/downloads.html 下载flink.
下载好Flink之后上传到Master(node1)节点上解压。
配置../conf/flink-conf.yaml
配置../conf/slaves
将配置好的Flink发送到其他worker节点上。
启动Flink集群，访问webui。  http://node01:8081
```

启动 Flink 集群

![](http://img.zwer.xyz/blog/20191225200007.png)

查看 node02上 TaskManager 节点

![](http://img.zwer.xyz/blog/20191225200053.png)

访问  Flink的 WebUI 

![](http://img.zwer.xyz/blog/20191225200157.png)

### 提交任务到集群中运行

- 将写好的项目使用 Maven 做打包处理上传到 Linux 服务器上

```java

/**
 * @Auther:zwer
 * @Date:2019/12/25 20:13
 * @Description:com.szxy
 * @Version:1.0
 * 需求： 接收 nc -lc 9999 Socket 数据
 * 将接收到的数据进行处理,将一行数据按照单词切分，并以（xxx,1）形式输出
 **/
public class FlinkStreamSocketDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();
        // 解析 args 参数
        ParameterTool parameterTool = ParameterTool.fromArgs(args);
        String node = "";
        int port = 0;
        if(parameterTool.has("node") && parameterTool.has("port")){
            node = parameterTool.get("node");
            port = Integer.valueOf(parameterTool.get("port"));
        }else{
            System.out.println("== 需要提供 node 和 port 参数 ==");
            System.exit(1);  // 程序终止运行， 退出 JVM
        }

        DataStreamSource<String> dataStreamSource = streamExecutionEnvironment.socketTextStream(node, port);
        SingleOutputStreamOperator<String> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String line, Collector<String> collector) throws Exception {
                String[] split = line.split(" ");
                for (String s : split) {
                    collector.collect(s);
                }
            }
        });
        SingleOutputStreamOperator<Tuple3<String, String, Integer>> map = flatMap.map(new MapFunction<String, Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> map(String s) throws Exception {
                return new Tuple3<>(s, s, 1);
            }
        });
        KeyedStream<Tuple3<String, String, Integer>, Tuple> keyBy = map.keyBy(0, 1);
        SingleOutputStreamOperator<Tuple3<String, String, Integer>> sumResult = keyBy.sum(2);
        sumResult.print();
        // 执行
        streamExecutionEnvironment.execute("socketProjectJob");

    }
}

```

- 提交 Flink 任务的命令

```
flink run --class com.szxy.FlinkStreamSocketDemo flik-workcount-demo-1.0-SNAPSHOT-jar-with-dependencies.jar --node node04 -port 9999

```

## Flink 流式处理窗口操作

### 案例

```java

/**
 * @Auther:zwer
 * @Date:2019/12/26 9:36
 * @Description:com.szxy
 * @Version:1.0
 * Flink 窗口操作
 * 需求：每隔一定时间 t 计算一批最近产生的 socket 数据
 **/
public class FlinkWindowWordCountDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStreamSource<String> dataStreamSource = env.socketTextStream("node04", 9999);
        SingleOutputStreamOperator<String> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String line, Collector<String> collector) throws Exception {
                String[] split = line.split(" ");
                for (String word : split) {
                    collector.collect(word);
                }
            }
        });

        SingleOutputStreamOperator<Tuple2<String, Integer>> map = flatMap.map(new MapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(String s) throws Exception {
                return new Tuple2<String, Integer>(s, 1);
            }
        });

        KeyedStream<Tuple2<String, Integer>, Tuple> keyBy = map.keyBy(0);
        // 每隔 1 秒计算最近 5 秒的产生的 socket 数据
        WindowedStream<Tuple2<String, Integer>, Tuple, TimeWindow> timeWindow =
                keyBy.timeWindow(Time.seconds(5), Time.seconds(1));
        SingleOutputStreamOperator<Tuple2<String, Integer>> result = timeWindow.sum(1);
        result.print();  // 控制台打印输出结果

        env.execute("job");
    }
}
```

## Flink 指定虚拟 Key

>  **Flink 批处理使用指定虚拟 key 使用  groupby方法, 而 Flink 流处理使用指定虚拟 key 使用 keyby 方法**

### 使用 tuple 指定虚拟 key

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 10:45
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 * 根据 Tuple指定虚拟 key
 * 注意： Flink 批处理使用指定虚拟 key 使用  groupby方法
 * 而 Flink 流处理使用指定虚拟 key 使用 keyby 方法
 **/
public class FlinkTupleVirtualKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/vkdata.txt");

        MapOperator<String, Tuple3<String, String, Integer>> map = dataSource.map(new MapFunction<String, Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> map(String s) throws Exception {
                String[] split = s.split(" ");
                return new Tuple3<String, String, Integer>(split[0], split[1], Integer.valueOf(split[2]));
            }
        });

        // 指定 tuple3 中第一个元素为虚拟 key
        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(0);
        // 指定 tuple3 中第二个元素为虚拟 key
//        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(1);
        // 指定 tuple3 中第一个元素和第三个元素为虚拟 key 
//        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(0,2);

        ReduceOperator<Tuple3<String, String, Integer>> result = groupBy.reduce(new ReduceFunction<Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> reduce(Tuple3<String, String, Integer> t1, Tuple3<String, String, Integer> t2) throws Exception {
                return new Tuple3<>(t1.f0 + t2.f0, t1.f1 + t2.f1, t1.f2 + t2.f2);
            }
        });

        result.print();
    }
}

```

###  使用Field Expression来指定key

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 11:14
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 * 使用字段作为虚拟 key
 *
 **/
public class FlinkUseFieldsVirtualKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/studentInfo.txt");
        MapOperator<String, StudentInfo> studentInfoMap = dataSource.map(new MapFunction<String, StudentInfo>() {
            @Override
            public StudentInfo map(String line) throws Exception {
                String[] arr = line.split(" ");
                String sname = arr[0];
                String gender = arr[1];
                int age = Integer.valueOf(arr[2]);
                Tuple2<Integer, Tuple3<Float, Float, Float>> genderAndScore =
                        new Tuple2<>(Integer.valueOf(arr[3]),
                                new Tuple3<>(Float.valueOf(arr[4]), Float.valueOf(arr[5]), Float.valueOf(arr[6])));
                return new StudentInfo(sname, gender, age, genderAndScore);
            }
        });

        // 使用 StudentInfo 中 gender 字段作为虚拟 key
//        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gender");
//        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gradeAndscore.1.2");
        // 使用 gradeAndscore 中的英语成绩字段作为虚拟 key 
        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gradeAndscore.f1.f2");


        ReduceOperator<StudentInfo> result = groupByGender.reduce(new ReduceFunction<StudentInfo>() {
            @Override
            public StudentInfo reduce(StudentInfo s1, StudentInfo s2) throws Exception {
                return new StudentInfo(s1.sname + "-" + s2.sname, s1.gender + "-" + s2.gender, s1.age + s2.age,
                        new Tuple2<>(s1.gradeAndscore.f0 + s2.gradeAndscore.f0,
                                new Tuple3<>(s1.gradeAndscore.f1.f0 + s2.gradeAndscore.f1.f0,
                                        s1.gradeAndscore.f1.f1 + s2.gradeAndscore.f1.f1,
                                        s1.gradeAndscore.f1.f2 + s2.gradeAndscore.f1.f2)));
            }
        });
        // 触发执行
        result.print();
    }


    /**
     *   类的访问级别必须为 public
     *   类必须显式写出默认构造方法
     *   若类的字段被 private 修饰符修饰，则必须提供 setter 和 getter 方法
     */
    public static class StudentInfo{
        private String sname;
        private String gender;
        private int age;
        // gender  <Chinese,Math,English>
        private Tuple2<Integer, Tuple3<Float,Float,Float>> gradeAndscore;

        public StudentInfo(){}

        public StudentInfo(String sname,String gender,int age,Tuple2<Integer,Tuple3<Float,Float,Float>> graderAndscore){
            this.sname = sname;
            this.age = age;
            this.gender = gender;
            this.gradeAndscore = graderAndscore;
        }

        public String getSname() {
            return sname;
        }

        public void setSname(String sname) {
            this.sname = sname;
        }

        public String getGender() {
            return gender;
        }

        public void setGender(String gender) {
            this.gender = gender;
        }

        public int getAge() {
            return age;
        }

        public void setAge(int age) {
            this.age = age;
        }

        public Tuple2<Integer, Tuple3<Float, Float, Float>> getGradeAndscore() {
            return gradeAndscore;
        }

        public void setGradeAndscore(Tuple2<Integer, Tuple3<Float, Float, Float>> gradeAndscore) {
            this.gradeAndscore = gradeAndscore;
        }

        @Override
        public String toString() {
            return "StudentInfo{" +
                    "sname='" + sname + '\'' +
                    ", gender='" + gender + '\'' +
                    ", age=" + age +
                    ", gradeAndscore=" + gradeAndscore +
                    '}';
        }
    }
}


```

### 使用Key Selector Functions来指定key

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 15:05
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 **/
public class FlinkKeySelectorVirutalKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/studentInfo.txt");

        /**
         *  使用 KeySelector 指定虚拟 key
         */
        UnsortedGrouping<String> groupByGender = dataSource.groupBy(new KeySelector<String, String>() {
            @Override
            public String getKey(String s) throws Exception {
                return s.split(" ")[1];  // 按照拆分第二个字段
            }
        });

        ReduceOperator<String> result = groupByGender.reduce(new ReduceFunction<String>() {
            @Override
            public String reduce(String s, String t1) throws Exception {
                return s + "#" + t1;
            }
        });
        // 触发执行
        result.print();
    }
}

```

## Flink 与 Kafka整合

### Flink 读取 Kafka 中数据 jar 依赖

```xml
<!-- Flink Kafka连接器的依赖-->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.11_2.11</artifactId>
    <version>1.7.1</version>
</dependency>
```

### Flink 读取 Kafka 中数据 offset 相关配置

- Apache Flink offset 配置

```
Flink消费kafka数据起始offset配置
flinkKafkaConsumer.setStartFromEarliest()
从topic的最早offset位置开始处理数据，如果kafka中保存有消费者组的消费位置将被忽略。

2.	flinkKafkaConsumer.setStartFromLatest()
从topic的最新offset位置开始处理数据，如果kafka中保存有消费者组的消费位置将被忽略。

3.	flinkKafkaConsumer.setStartFromTimestamp(…)
从指定的时间戳（毫秒）开始消费数据，Kafka中每个分区中数据大于等于设置的时间戳的数据位置将被当做开始消费的位置。如果kafka中保存有消费者组的消费位置将被忽略。

4.	flinkKafkaConsumer.setStartFromGroupOffsets()
默认的设置。根据代码中设置的group.id设置的消费者组，去kafka中或者zookeeper中找到对应的消费者offset位置消费数据。如果没有找到对应的消费者组的位置，那么将按照auto.offset.reset设置的策略读取offset。

```

- Flink消费kafka数据，消费者offset提交配置

```java
Flink提供了消费kafka数据的offset如何提交给Kafka或者zookeeper(kafka0.8之前)的配置。注意，Flink并不依赖提交给Kafka或者zookeeper中的offset来保证容错。提交的offset只是为了外部来查询监视kafka数据消费的情况。

配置offset的提交方式取决于是否为job设置开启checkpoint。可以使用env.enableCheckpointing(5000)来设置开启checkpoint。
```

- Apache Flink offset 提交配置

```java
关闭checkpoint：
	如何禁用了checkpoint，那么offset位置的提交取决于Flink读取kafka客户端的配置，enable.auto.commit ( auto.commit.enable【Kafka 0.8】)配置是否开启自动提交offset, auto.commit.interval.ms决定自动提交offset的周期。

开启checkpoint：
	如果开启了checkpoint，那么当checkpoint保存状态完成后，将checkpoint中保存的offset位置提交到kafka。这样保证了Kafka中保存的offset和checkpoint中保存的offset一致，可以通过配置setCommitOffsetsOnCheckpoints(boolean)来配置是否将checkpoint中的offset提交到kafka中（默认是true）。如果使用这种方式，那么properties中配置的kafka offset自动提交参数enable.auto.commit和周期提交参数auto.commit.interval.ms参数将被忽略。
```

### Flink 读取 Kafka 中数据

需求： Flink 读取 Kafka 中的数据，进行数据处理后，再将数据写回 Kafka 中

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 15:34
 * @Description:com.szxy.flinkkafka
 * @Version:1.0
 * Flink 从 kafka 中 flinkTopic 读取数据，将数据处理后，再写入 kafka 中 ResultFlinkTopic 中
 * eg: 输入数据： 1 2 3
 *     输出数据： 1-1
 *               2-1
 *               3-1
 **/
public class FlinkKafkaDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置并行度为 1,默认的并行度为 8 
        env.setParallelism(1);

        Properties props = new Properties();
        props.setProperty("bootstrap.servers","node02:9092,noed03:9092,node04:9092");
        props.setProperty("group.id","myflinkgroup");
        props.setProperty("auto.offset.reset","earliest");
        /**
         *  第一个参数表示： kafka 中 topic 的名称，由 flink 自动创建，无需手动创建
         *  第二个参数表示:  value 的反序列化格式
         *  第三个参数表示:  kafka 配置信息
         */
        FlinkKafkaConsumer011<String> flinkTopic = new FlinkKafkaConsumer011<>("flinkTopic", new SimpleStringSchema(), props);
        // 从 topic 中最早的 offset 开始读取
        flinkTopic.setStartFromEarliest();
        // 消费 kafka 中的数据，也就是把 kafka 中消息作为数据源读取
        DataStreamSource<String> dataStreamSource = env.addSource(flinkTopic);

        /**
         *  将进来的一条数据转为 Tuple2 类型数据输出
         *  String => Tuple2<String, Integer>
         */
        SingleOutputStreamOperator<Tuple2<String, Integer>> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
                String[] split = s.split(" ");
                for (String s1 : split) {
                    collector.collect(new Tuple2<>(s1, 1));
                }
            }
        });

        KeyedStream<Tuple2<String, Integer>, Tuple> keyBy = flatMap.keyBy(0);
        SingleOutputStreamOperator<Tuple2<String, Integer>> sumResult = keyBy.sum(1);
        /**
         *  第一个参数表示： kafka 中 topic 的名称，由 flink 自动创建，无需手动创建
         *  第二个参数表示:  value 的反序列化格式
         *  第三个参数表示:  kafka 配置信息
         *  注意： FlinkKafkaProducer011 继承 TwoPhaseCommitSinkFunction 实现两阶段提交
         *  也就说 Flink 把数据写往外部的 kafka 中不需要担心数据不一致问题
         */
        FlinkKafkaProducer011<String> resultFlinkTopic = new FlinkKafkaProducer011<>("ResultFlinkTopic", new SimpleStringSchema(), props);
        /**
         *  将Tuple2<String,Integer> 格式的数据转为一条 String 类型的数据
         *  (String,Integer) => "String-Integer"
         */
         sumResult.map(new MapFunction<Tuple2<String, Integer>, String>() {
            @Override
            public String map(Tuple2<String, Integer> t2) throws Exception {
                return t2.f0 +"-"+ t2.f1;
            }
        }).addSink(resultFlinkTopic); // 将数据处理结果写回 Kafka 中

         // 触发执行
         env.execute("flinkKafkaJob");
    }
}
```

### Apache Flink +Kafka仅一次消费数据

> 使用checkpoint + 两阶段提交来保证仅一次消费kafka中的数据

![](http://img.zwer.xyz/blog/两阶段提交保证数据容错.jpg)

```
Flink中外部状态实现两阶段提交

Flink外部状态实现两阶段提交将逻辑封装到TwoPhaseComitSinkFunction类中，需要扩展TwoPhaseCommitSinkFunction来实现仅一次消费数据。若要实现支持exactly-once语义的自定义sink,需要实现以下4个方法：

1.	beginTransaction:开启一个事务，创建一个临时文件，将数据写入到临时文件中
2.	preCommit:在pre-commit阶段，flush缓存数据到磁盘，然后关闭这个文件，确保不会	有新的数据写入到这个文件，同时开启一个新事务执行属于下一个checkpoint的写入	操作。
3.	commit：在commit阶段，我们以原子性的方式将上一阶段的文件写入真正的文件目	录下。【注意：数据有延时，不是实时的】。
4.	abort:一旦异常终止事务，程序如何处理。这里要清除临时文件。

```

![](http://img.zwer.xyz/blog/20191227153558.png)



示例代码： 需求：Flink 从 kafka 读取数据，处理完数据将结果在控制台打印，要求：数据有且只有一次被正确消费，也就是说数据不能消费多次。

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 17:09
 * @Description:com.szxy.flinkkafka
 * @Version:1.0
 * 从 kafka 中读取数据，
 * 通过窗口方式，数据处理后保存到外部文件中
 **/
public class ReadMessageFromKafkaUseFileSave {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        /**
         *  设置检查点
         */
        env.enableCheckpointing(5000);

        /**
         *  kafka 配置信息
         */
        Properties props = new Properties();
        props.setProperty("bootstrap.servers","node02:9092,node03:9092,node04:9092");
        props.setProperty("group.id","myConsumerGroupId");
        props.setProperty("auto.offset.reset","earliest");
        /**
         * 如果开启了checkpoint，那么当checkpoint保存状态完成后，将checkpoint中保存的offset位置提交到kafka。
         * 这样保证了Kafka中保存的offset和checkpoint中保存的offset一致，可以通过配置setCommitOffsetsOnCheckpoints(boolean)
         * 来配置是否将checkpoint中的offset提交到kafka中（默认是true）。
         * 如果使用这种方式，那么properties中配置的kafka offset自动提交参数enable.auto.commit和周期提交参数auto.commit.interval.ms参数将被忽略。
         */
        props.setProperty("enable.auto.commit","false");


        FlinkKafkaConsumer011<String> flinkReadMSGTopic = new FlinkKafkaConsumer011<>("flinkReadMSGTopic", new SimpleStringSchema(), props);

        /**
         * 从topic的最早offset位置开始处理数据，如果kafka中保存有消费者组的消费位置将被忽略。
         */
        // flinkReadMSGTopic.setStartFromEarliest();
        /**
         *  从topic的最新offset位置开始处理数据，如果kafka中保存有消费者组的消费位置将被忽略。
         */
        // flinkReadMSGTopic.setStartFromLatest();
        /**
         * 从指定的时间戳（毫秒）开始消费数据，Kafka中每个分区中数据大于等于设置的时间戳的数据位置将被当做开始消费的位置。
         * 如果kafka中保存有消费者组的消费位置将被忽略。
         */
        // flinkReadMSGTopic.setStartFromTimestamp(…);
        /**
         * 默认的设置。根据代码中设置的group.id设置的消费者组，去kafka中或者zookeeper中找到对应的消费者offset位置消费数据。
         * 如果没有找到对应的消费者组的位置，那么将按照auto.offset.reset设置的策略读取offset。
         */
        flinkReadMSGTopic.setStartFromGroupOffsets(); // 默认设置
        /**
         * 配置是否将checkpoint中的offset提交到kafka中（默认是true）。
         */
        flinkReadMSGTopic.setCommitOffsetsOnCheckpoints(true);

        DataStreamSource<String> dataStreamSource = env.addSource(flinkReadMSGTopic);

        SingleOutputStreamOperator<Tuple2<String, Integer>> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
                String[] split = s.split(" ");
                for (String s1 : split) {
                    collector.collect(new Tuple2<>(s1, 1));
                }
            }
        });

        KeyedStream<Tuple2<String, Integer>, Tuple> keyBy = flatMap.keyBy(0);
        WindowedStream<Tuple2<String, Integer>, Tuple, TimeWindow> timeWindow = keyBy.timeWindow(Time.seconds(5));
        SingleOutputStreamOperator<Tuple2<String, Integer>> sumResult = timeWindow.sum(1);

        // 将 tuple2 数据格式转为 String 格式
        SingleOutputStreamOperator<String> map = sumResult.map(new MapFunction<Tuple2<String, Integer>, String>() {
            @Override
            public String map(Tuple2<String, Integer> t1) throws Exception {
                return t1.f0 + "#" + t1.f1;
            }
        });

        map.addSink(new MyTransactionSink()).setParallelism(1); // 设置并行度为 1

        env.execute("ReadMsgFromKafkaJob");

    }
}
```

自定义两阶段提交 Sink 

```java
package com.szxy.flinkkafka;

import org.apache.flink.api.common.ExecutionConfig;
import org.apache.flink.api.common.typeutils.base.VoidSerializer;
import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
import org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction;

import java.io.Serializable;
import java.util.*;

import static org.apache.flink.util.Preconditions.*;

/**
 * @Auther:zwer
 * @Date:2019/12/26 22:25
 * @Description:com.szxy.flinkkafka
 * @Version:1.0
 *  Flink 两阶段提交
 * Flink 将数据处理后，保存数据到外部文件中
 **/
public class MyTransactionSink extends TwoPhaseCommitSinkFunction<String, MyTransactionSink.ContentTransaction, Void> {


    private ContentBuffer contentBuffer = new ContentBuffer();

    public MyTransactionSink(){
        super(new KryoSerializer<>(ContentTransaction.class, new ExecutionConfig()), VoidSerializer.INSTANCE);
    }

    /**
     * 第一步执行
     * 开启一个事务， 在临时目录下从创建一个临时文件，之后写入数据到个该文件
     * @return
     * @throws Exception
     */
    @Override
    protected ContentTransaction beginTransaction() throws Exception {
        ContentTransaction contentTransaction =
                new ContentTransaction(contentBuffer.createBuffer(UUID.randomUUID().toString()));
        System.out.println("==== beginTransaction===,contentTransaction Name="+contentTransaction.toString());
        return contentTransaction;
    }

    /**
     * 第二步执行
     * 当有数据时，会执行到这个 invoke 方法
     * @param transaction   事务对象
     * @param value         数据内容
     * @param context
     * @throws Exception
     */
    @Override
    protected void invoke(ContentTransaction transaction, String value, Context context) throws Exception {
        System.out.println("=== invoke ===="+value);
        transaction.tempContentWriter.write(value);
        System.out.println("invoke---正在休息 5s ");
        Thread.sleep(5000);
        System.out.println("invoke---休息完成 5s");
    }

    /**
     * 第三步执行
     * 在 pre-commit 阶段， flush 缓存数据块到磁盘
     * 然后关闭该文件， 确保不再写入新数据到该文件
     * 同时开启一个新事务执行属于下一个 checkPoint 的写入操作
     * @param transaction 事务对象
     * @throws Exception
     */
    @Override
    protected void preCommit(ContentTransaction transaction) throws Exception {
        System.out.println("=== preCommit ===,contentTransaction Name = "+transaction.toString());
        transaction.tempContentWriter.flush();
        transaction.tempContentWriter.close();
    }

    /**
     * 第四步执行
     * 在 commit 阶段，我们以原子性的方式将上一阶段的文件
     * 写入真正的文件目录下。 这里有延迟
     * @param transaction
     */
    @Override
    protected void commit(ContentTransaction transaction) {
        // 打印事务对象信息
        System.out.println("=== commit ===,contentTransaction Name = "+transaction.toString());

        /**
         *  实现写入文件的逻辑
         */
        // 获取名称，即事务 ID 
        String name = transaction.tempContentWriter.getName();
        // 获取数据
        Collection<String> content = contentBuffer.read(name);

        /**
         * 测试打印
         */
        for (String s : content) {
            System.out.println(s);
            // TODO: 2019/12/27 将数据处理结果写入外部文件中

        }
    }

    /**
     * 一旦有异常终止事务时， 删除临时文件
     * @param transaction
     */
    @Override
    protected void abort(ContentTransaction transaction) {
        System.out.println("=== abort ===");
        transaction.tempContentWriter.close();
        contentBuffer.delete(transaction.tempContentWriter.getName());
    }

    /**
     * 内容事务
     */
    public static  class ContentTransaction{
        private ContentBuffer.TempContentWriter tempContentWriter;

        public ContentTransaction(ContentBuffer.TempContentWriter tempContentWriter){
            this.tempContentWriter = tempContentWriter;
        }

        @Override
        public String toString() {
            return "ContentTransaction{" + "tempContentWriter=" + tempContentWriter + '}';
        }
    }
}

/**
 * ContentBuffer 类中放临时处理数据到一个 List 中
 */
class ContentBuffer implements Serializable {

    private Map<String, List<String>> filesContent = new HashMap<>();

    /**
     * 创建一个 Buffer
     * @param name   UUID 字符串作为事务 ID     UUID.randomUUID().ToString
     * @return
     */
    public TempContentWriter createBuffer(String name){
        checkArgument(!filesContent.containsKey(name), "File [$s] already exists", name);
        filesContent.put(name, new ArrayList<>());
        return new TempContentWriter(name, this);
    }

    public void putContent(String name, List<String> values) {
        List<String> content = filesContent.get(name);
        checkState(content != null, "Unknown file [%s]", name);
        content.addAll(values);
    }

    public Collection<String> read(String name){
        List<String> content = filesContent.get(name);
        checkState(content != null, "UnKnown file [%s] ", name);
        List<String> result = new ArrayList<>(content);
        return result;
    }

    public void delete(String name){
        filesContent.remove(name);
    }

    // 内部类
    class TempContentWriter {
        private final ContentBuffer contentBuffer;
        private final String name; // 事务 Id
        private final List<String> buffer = new ArrayList<>(); // 存放数据
        private boolean closed = false;  // 是否关闭事务

        public String getName() {
            return name;
        }

        public TempContentWriter(String name, ContentBuffer contentBuffer) {
            this.name = checkNotNull(name);
            this.contentBuffer = checkNotNull(contentBuffer);
        }

        public TempContentWriter write(String value) {
            checkState(!closed);
            buffer.add(value);
            return this;
        }

        public TempContentWriter flush() {
            contentBuffer.putContent(name, buffer);
            return this;
        }

        public void close() {
            buffer.clear();
            closed = true;
        }
    }

}
```

需要注意的是在 Flink 中 `FlinkKafkaProducer011`  类中已经实现` TwoPhaseCommitSinkFunction<> `两阶段提交,即 Flink 处理完数据再将数据写往外部 Kafka 消息主题中是不需要关心消息是否仅一次消费数据的问题

```java
public class FlinkKafkaProducer011<IN>   extends TwoPhaseCommitSinkFunction<IN, FlinkKafkaProducer011.KafkaTransactionState, FlinkKafkaProducer011.KafkaTransactionContext> {
```

