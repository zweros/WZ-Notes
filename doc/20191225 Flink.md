---
title: 20191225 Flink
date: 2019-12-25
---
#  Flink
[TOC]



## Flink 概念

> **Apache Flink® — Stateful Computations over Data Streams**
>
> 翻译过来的意思是基于数据流的有状态计算

 ![](https://flink.apache.org/img/flink-home-graphic.png)

## Storm & SparkStreaming  & Flink 对比

|                | 处理数据的方式         | 延时度 | 吞吐量 |
| -------------- | ---------------------- | ------ | ------ |
| Storm          | 实时处理               | 低     | 低     |
| SparkStreaming | 微批流式处理           | 高     | 高     |
| Flink          | 支持实时处理的流式处理 | 低     | 高     |

## Flink 版本 WordCount

### 搭建 Flink 的Maven 环境

- pom.xml 

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  
  <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
  
    <groupId>com.szxy</groupId>
    <artifactId>flik-workcount-demo</artifactId>
    <version>1.0-SNAPSHOT</version>
  
    <name>flik-workcount-demo</name>
    <!-- FIXME change it to the project's website -->
    <url>http://www.example.com</url>
  
    <properties>
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
      <maven.compiler.source>1.8</maven.compiler.source>
      <maven.compiler.target>1.8</maven.compiler.target>
      <flink.version>1.7.1</flink.version>
    </properties>
  
    <dependencies>
      <!-- Flink 依赖 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-java</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-wikiedits_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <!-- Flink Kafka连接器的依赖 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka-0.11_2.11</artifactId>
        <version>${flink.version}</version>
      </dependency>
      <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.11</version>
        <scope>test</scope>
      </dependency>
  
      <!-- Flink Scala2.11 版本 -->
      <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-scala_2.11</artifactId>
        <version>1.7.1</version>
      </dependency>
  
      <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>2.11.12</version>
      </dependency>
  
     <!-- log4j 和slf4j 包,如果在控制台不想看到日志，可以将下面的包注释掉-->
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.25</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>1.2.17</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>1.7.25</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-nop</artifactId>
        <version>1.7.25</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>1.7.5</version>
      </dependency>
    </dependencies>
  
    <build>
      <plugins>
        <!-- 在maven项目中既有java又有scala代码时配置 maven-scala-plugin 插件打包时可以将两类代码一起打包 -->
        <plugin>
          <groupId>org.scala-tools</groupId>
          <artifactId>maven-scala-plugin</artifactId>
          <version>2.15.2</version>
          <executions>
            <execution>
              <goals>
                <goal>compile</goal>
                <goal>testCompile</goal>
              </goals>
            </execution>
          </executions>
        </plugin>
        <plugin>
          <artifactId>maven-assembly-plugin</artifactId>
          <version>2.4</version>
          <configuration>
            <!-- 设置false后是去掉 MySpark-1.0-SNAPSHOT-jar-with-dependencies.jar 后的 “-jar-with-dependencies” -->
            <!--<appendAssemblyId>false</appendAssemblyId>-->
            <descriptorRefs>
              <descriptorRef>jar-with-dependencies</descriptorRef>
            </descriptorRefs>
            <archive>
              <manifest>
                <mainClass>com.lw.myflink.Streaming.FlinkSocketWordCount</mainClass>
              </manifest>
            </archive>
          </configuration>
          <executions>
            <execution>
              <id>make-assembly</id>
              <phase>package</phase>
              <goals>
                <goal>assembly</goal>
              </goals>
            </execution>
          </executions>
        </plugin>
      </plugins>
    </build>
  </project>
  ```

### 编写单词统计  Java 代码

- 搭建 Flink 批处理环境

  ```java
  // 创建 Flink 流式处理执行环境
  //StreamExecutionEnvironment execStreamEvn = 		StreamExecutionEnvironment.getExecutionEnvironment();
  ```

- 搭建 Flink 流式处理环境

  ```java
  // 创建 Flink 批处理执行环境
  ExecutionEnvironment execEvn = ExecutionEnvironment.getExecutionEnvironment();
  ```

- FlinkWordCount.java

  ```java
  package com.szxy;
  
  import org.apache.flink.api.common.functions.FlatMapFunction;
  import org.apache.flink.api.common.functions.MapFunction;
  import org.apache.flink.api.java.ExecutionEnvironment;
  import org.apache.flink.api.java.operators.*;
  import org.apache.flink.api.java.tuple.Tuple2;
  import org.apache.flink.util.Collector;
  
  /**
   * @Auther:zwer
   * @Date:2019/12/25 15:23
   * @Description:com.szxy
   * @Version:1.0
   * Flink WordCount 单词个数统计
   **/
  public class FlikWordCountDemo {
  
      public static void main(String[] args) throws Exception {
          // 创建 Flink 执行环境
          ExecutionEnvironment execEvn = ExecutionEnvironment.getExecutionEnvironment();
          DataSource<String> dataSource = execEvn.readTextFile("F:\\workspacesIDEA\\flik-workcount-demo\\data\\words.txt");
          // 拆分单词    "hello World" => "hello","word"
          FlatMapOperator<String, String> words = dataSource.flatMap(new FlatMapFunction<String, String>() {
              @Override
              public void flatMap(String line, Collector<String> collector) throws Exception {
                  String[] split = line.split(" ");
                  for (String s : split) {
                      collector.collect(s);
                  }
              }
          });
          // 为单词，标注数字  "word"=>"(word,1)"
          // 注意：这里的 Tuple2<T,T> 使用是 Flink 包下的
          MapOperator<String, Tuple2<String, Integer>> pairWord = words.map(new MapFunction<String, Tuple2<String, Integer>>() {
              @Override
              public Tuple2<String, Integer> map(String word) throws Exception {
                  return new Tuple2(word, 1);
              }
          });
          // 统计每个单词出现的次数
          UnsortedGrouping<Tuple2<String, Integer>> groupBy = pairWord.groupBy(0);
          AggregateOperator<Tuple2<String, Integer>> result = groupBy.sum(1);
          result.print();
  
      }
  
  }
  
  ```

### Flink 处理数据流程 & 代码执行流程

- Flink处理数据流程
  	Source -> Transformations  -> Sink
    	数据源头  ->  数据转换  -> 数据输出 

- Flink程序的执行过程：

  ```
  获取flink的执行环境(execution environment)
  加载数据-- soure
  对加载的数据进行转换 -- transformation
  对结果进行保存或者打印 --sink
  触发flink程序的执行(execute(),count(),collect(),print())，例如：调用ExecutionEnvironment或者StreamExecutionEnvironment的execute()方法。
  ```

### 注意事项

1. `Flink` 不是基于K,V 格式处理数据，但可以指定某些虚拟 Key 
2. `Flink` 中批处理的数据对象是 `DataSet`, 而流式处理的数据对象是 `DataStream`
3. `Flink` 的代码流程必须符合 `Source -> Transformaction -> sink` 。transformation 都是懒执行，需要最后使用 `env.execute() `触发执行或者会用` print()`、`count()`、`collect()`触发执行

4. Flink 中 Tuple 支持25个元素，下标从 0 开始

## Flink 中全局排序 & 分区排序

在客户端 Eclipse 测试中 Flink 默认使用 8 个分区

- 设置 Flink 并行度

```
//  设置并行度，默认的并行度为 8
execEvn.setParallelism(1);
```

- 排序

  ```java
  // 统计每个单词出现的次数
  UnsortedGrouping<Tuple2<String, Integer>> groupBy = pairWord.groupBy(0);
  AggregateOperator<Tuple2<String, Integer>> result = groupBy.sum(1);
  // 分区内排序
  SortPartitionOperator<Tuple2<String, Integer>> sortResult =
      result.sortPartition(1, Order.DESCENDING);
  sortResult.print();
  ```

当 Flink 并行度为  1 时，分区内排序恰好时全局排序

## Flink  Source & Sink

### Source  

```

```

### Sink

```

```

## Flink 累加器  & 计数器

计算器是累加器的具体实现

计数器有：` IntCounter` 、`DoubleCounter`、`LongCounter ` 

```java
package com.szxy;

import org.apache.flink.api.common.JobExecutionResult;
import org.apache.flink.api.common.accumulators.IntCounter;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.DataSink;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.operators.FilterOperator;
import org.apache.flink.api.java.operators.MapOperator;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.core.fs.FileSystem;

/**
 * @Auther:zwer
 * @Date:2019/12/25 17:28
 * @Description:com.szxy
 * @Version:1.0
 * Flink 中累加器的使用， 累加器在 Flink 中具体表现为计数器
 * 需求： 读取文本文件，统计其中以 https:// 开头的网址个数（默认一个网址占据一行）
 **/
public class FlinkAccumulatorDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment execEnv = ExecutionEnvironment.getExecutionEnvironment();
        execEnv.setParallelism(1);
        DataSource<String> dataSource = execEnv.readTextFile("./data/website.txt");
        // 过滤以 https:// 开头的数据
        FilterOperator<String> filterSites = dataSource.filter(new FilterFunction<String>() {
            @Override
            public boolean filter(String s) throws Exception {
                return s.startsWith("https://");
            }
        });
        /**
         *  若要使用累加器，需要使用  RichMapFunction<String,T>
         */
        MapOperator<String, String> mapSites = filterSites.map(new RichMapFunction<String, String>() {
            /**
             *   int 类型的计算器
             */
            private IntCounter counter = new IntCounter();

            /**
             * 注册累加器
             * @param parameters
             * @throws Exception
             */
            @Override
            public void open(Configuration parameters) throws Exception {
                // 注册累加器  第一个参数表示累计器的名称，第二个参数表示累计器具体实现类型
                getRuntimeContext().addAccumulator("myAcc", counter);
            }

            /**
             * 计算
             * @param s  一行数据
             * @return s  一行数据
             * @throws Exception
             */
            @Override
            public String map(String s) throws Exception {
                counter.add(1); // 每遍历一个数据，统计的个数加 1
                return s;
            }
        });

        DataSink<String> dataSink =
                mapSites.writeAsText("./data/result/websitesCountResult.txt", FileSystem.WriteMode.OVERWRITE);
        JobExecutionResult webSiteJob = execEnv.execute("countWebSiteJob");
        int myAcc = webSiteJob.getAccumulatorResult("myAcc");
        System.out.println("myAcc of ressult = "+myAcc);
    }
}
```

## Flink 编程模型

- Flink中数据类型
  有界数据流  
  无界数据流

- Flink三种处理数据模型
   Flink批处理
  Flink批处理中处理的是有界数据流   --Dataset

- Flink流式处理
  Flink流式处理中有界数据流也有无界数据流  --DataStream

- FlinkSQL处理
  有界数据流也有无界数据流

## Apache Flink 术语

### Dataflows

![](http://img.zwer.xyz/blog/20191225192404.png)

### parallel Dataflow

![](http://img.zwer.xyz/blog/20191225192433.png)

### Task 和 算子链

![](http://img.zwer.xyz/blog/20191225192509.png)

### JobManager、TaskManager和clients:

Flink运行时包含两种类型的进程：

- JobManger:也叫作masters,协调分布式执行，调度task,协调checkpoint，协调故障恢复。在Flink程序中至少有一个JobManager,高可用可以设置多个JobManager,其中一个是Leader,其他都是standby状态。

- TaskManager:也叫workers,执行dataflow生成的task,负责缓冲数据，及TaskManager之间的交换数据。Flink程序中必须有一个TaskManager。

  Flink程序可以运行在standalone集群，Yarn或者Mesos资源调度框架中。

- clients不是Flink程序运行时的一部分，作用是向JobManager准备和发送dataflow,之后，客户端可以断开连接或者保持连接。

![](http://img.zwer.xyz/blog/20191225192539.png)

### TaskSlot 任务槽

每个Worker（TaskManager）是一个JVM进程，可以执行一个或者多个task,这些task可以运行在任务槽上，每个worker上至少有一个任务槽。每个任务槽都有固定的资源，例如：TaskManager有三个TaskSlots,那么每个TaskSlot会将TaskMananger中的内存均分，即每个任务槽的内存是总内存的1/3。**任务槽的作用就是分离任务的托管内存，不会发生cpu隔离。**
通过调整任务槽的数据量，用户可以指定每个TaskManager有多少任务槽，更多的任务槽意味着更多的task可以共享同一个JVM,同一个JVM中的task共享TCP连接和心跳信息，共享数据集和数据结构，从而减少TaskManager中的task开销。

总结：task slot的个数代表TaskManager可以并行执行的task数。

![](http://img.zwer.xyz/blog/20191225193421.png)



## Flink 集群搭建

### 节点分布

|             | node01 | node02 | node03 |
| ----------- | ------ | ------ | ------ |
| JobManager  | *      |        |        |
| TaskManager |        | *      | *      |

### Flink 集群搭建步骤

```shell
进入https://flink.apache.org/downloads.html 下载flink.
下载好Flink之后上传到Master(node1)节点上解压。
配置../conf/flink-conf.yaml
配置../conf/slaves
将配置好的Flink发送到其他worker节点上。
启动Flink集群，访问webui。  http://node01:8081
```

启动 Flink 集群

![](http://img.zwer.xyz/blog/20191225200007.png)

查看 node02上 TaskManager 节点

![](http://img.zwer.xyz/blog/20191225200053.png)

访问  Flink的 WebUI 

![](http://img.zwer.xyz/blog/20191225200157.png)

### 提交任务到集群中运行

- 将写好的项目使用 Maven 做打包处理上传到 Linux 服务器上

```java

/**
 * @Auther:zwer
 * @Date:2019/12/25 20:13
 * @Description:com.szxy
 * @Version:1.0
 * 需求： 接收 nc -lc 9999 Socket 数据
 * 将接收到的数据进行处理,将一行数据按照单词切分，并以（xxx,1）形式输出
 **/
public class FlinkStreamSocketDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();
        // 解析 args 参数
        ParameterTool parameterTool = ParameterTool.fromArgs(args);
        String node = "";
        int port = 0;
        if(parameterTool.has("node") && parameterTool.has("port")){
            node = parameterTool.get("node");
            port = Integer.valueOf(parameterTool.get("port"));
        }else{
            System.out.println("== 需要提供 node 和 port 参数 ==");
            System.exit(1);  // 程序终止运行， 退出 JVM
        }

        DataStreamSource<String> dataStreamSource = streamExecutionEnvironment.socketTextStream(node, port);
        SingleOutputStreamOperator<String> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String line, Collector<String> collector) throws Exception {
                String[] split = line.split(" ");
                for (String s : split) {
                    collector.collect(s);
                }
            }
        });
        SingleOutputStreamOperator<Tuple3<String, String, Integer>> map = flatMap.map(new MapFunction<String, Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> map(String s) throws Exception {
                return new Tuple3<>(s, s, 1);
            }
        });
        KeyedStream<Tuple3<String, String, Integer>, Tuple> keyBy = map.keyBy(0, 1);
        SingleOutputStreamOperator<Tuple3<String, String, Integer>> sumResult = keyBy.sum(2);
        sumResult.print();
        // 执行
        streamExecutionEnvironment.execute("socketProjectJob");

    }
}

```

- 提交 Flink 任务的命令

```
flink run --class com.szxy.FlinkStreamSocketDemo flik-workcount-demo-1.0-SNAPSHOT-jar-with-dependencies.jar --node node04 -port 9999

```

## Flink 流式处理窗口操作

### 案例

```java

/**
 * @Auther:zwer
 * @Date:2019/12/26 9:36
 * @Description:com.szxy
 * @Version:1.0
 * Flink 窗口操作
 * 需求：每隔一定时间 t 计算一批最近产生的 socket 数据
 **/
public class FlinkWindowWordCountDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStreamSource<String> dataStreamSource = env.socketTextStream("node04", 9999);
        SingleOutputStreamOperator<String> flatMap = dataStreamSource.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String line, Collector<String> collector) throws Exception {
                String[] split = line.split(" ");
                for (String word : split) {
                    collector.collect(word);
                }
            }
        });

        SingleOutputStreamOperator<Tuple2<String, Integer>> map = flatMap.map(new MapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(String s) throws Exception {
                return new Tuple2<String, Integer>(s, 1);
            }
        });

        KeyedStream<Tuple2<String, Integer>, Tuple> keyBy = map.keyBy(0);
        // 每隔 1 秒计算最近 5 秒的产生的 socket 数据
        WindowedStream<Tuple2<String, Integer>, Tuple, TimeWindow> timeWindow =
                keyBy.timeWindow(Time.seconds(5), Time.seconds(1));
        SingleOutputStreamOperator<Tuple2<String, Integer>> result = timeWindow.sum(1);
        result.print();  // 控制台打印输出结果

        env.execute("job");
    }
}
```

## Flink 指定虚拟 Key

使用Tuples来指定key
使用Field Expression来指定key
使用Key Selector Functions来指定key

### 使用 tuple 指定虚拟 key

 Flink 批处理使用指定虚拟 key 使用  groupby方法, 而 Flink 流处理使用指定虚拟 key 使用 keyby 方法

```java
/**
 * @Auther:zwer
 * @Date:2019/12/26 10:45
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 * 根据 Tuple指定虚拟 key
 * 注意： Flink 批处理使用指定虚拟 key 使用  groupby方法
 * 而 Flink 流处理使用指定虚拟 key 使用 keyby 方法
 **/
public class FlinkTupleVirtualKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/vkdata.txt");

        MapOperator<String, Tuple3<String, String, Integer>> map = dataSource.map(new MapFunction<String, Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> map(String s) throws Exception {
                String[] split = s.split(" ");
                return new Tuple3<String, String, Integer>(split[0], split[1], Integer.valueOf(split[2]));
            }
        });

        // 指定 tuple3 中第一个元素为虚拟 key
        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(0);
        // 指定 tuple3 中第二个元素为虚拟 key
//        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(1);
        // 指定 tuple3 中第一个元素和第三个元素为虚拟 key 
//        UnsortedGrouping<Tuple3<String, String, Integer>> groupBy = map.groupBy(0,2);

        ReduceOperator<Tuple3<String, String, Integer>> result = groupBy.reduce(new ReduceFunction<Tuple3<String, String, Integer>>() {
            @Override
            public Tuple3<String, String, Integer> reduce(Tuple3<String, String, Integer> t1, Tuple3<String, String, Integer> t2) throws Exception {
                return new Tuple3<>(t1.f0 + t2.f0, t1.f1 + t2.f1, t1.f2 + t2.f2);
            }
        });

        result.print();
    }
}

```

###  使用Field Expression来指定key

```java
package com.szxy.virtualkey;

import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.operators.MapOperator;
import org.apache.flink.api.java.operators.ReduceOperator;
import org.apache.flink.api.java.operators.UnsortedGrouping;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.api.java.tuple.Tuple3;

/**
 * @Auther:zwer
 * @Date:2019/12/26 11:14
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 * 使用字段作为虚拟 key
 *
 **/
public class FlinkUseFieldsVirtualKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/studentInfo.txt");
        MapOperator<String, StudentInfo> studentInfoMap = dataSource.map(new MapFunction<String, StudentInfo>() {
            @Override
            public StudentInfo map(String line) throws Exception {
                String[] arr = line.split(" ");
                String sname = arr[0];
                String gender = arr[1];
                int age = Integer.valueOf(arr[2]);
                Tuple2<Integer, Tuple3<Float, Float, Float>> genderAndScore =
                        new Tuple2<>(Integer.valueOf(arr[3]),
                                new Tuple3<>(Float.valueOf(arr[4]), Float.valueOf(arr[5]), Float.valueOf(arr[6])));
                return new StudentInfo(sname, gender, age, genderAndScore);
            }
        });

        // 使用 StudentInfo 中 gender 字段作为虚拟 key
//        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gender");
//        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gradeAndscore.1.2");
        // 使用 gradeAndscore 中的英语成绩字段作为虚拟 key 
        UnsortedGrouping<StudentInfo> groupByGender = studentInfoMap.groupBy("gradeAndscore.f1.f2");


        ReduceOperator<StudentInfo> result = groupByGender.reduce(new ReduceFunction<StudentInfo>() {
            @Override
            public StudentInfo reduce(StudentInfo s1, StudentInfo s2) throws Exception {
                return new StudentInfo(s1.sname + "-" + s2.sname, s1.gender + "-" + s2.gender, s1.age + s2.age,
                        new Tuple2<>(s1.gradeAndscore.f0 + s2.gradeAndscore.f0,
                                new Tuple3<>(s1.gradeAndscore.f1.f0 + s2.gradeAndscore.f1.f0,
                                        s1.gradeAndscore.f1.f1 + s2.gradeAndscore.f1.f1,
                                        s1.gradeAndscore.f1.f2 + s2.gradeAndscore.f1.f2)));
            }
        });
        // 触发执行
        result.print();
    }


    /**
     *   类的访问级别必须为 public
     *   类必须显式写出默认构造方法
     *   若类的字段被 private 修饰符修饰，则必须提供 setter 和 getter 方法
     */
    public static class StudentInfo{
        private String sname;
        private String gender;
        private int age;
        // gender  <Chinese,Math,English>
        private Tuple2<Integer, Tuple3<Float,Float,Float>> gradeAndscore;

        public StudentInfo(){}

        public StudentInfo(String sname,String gender,int age,Tuple2<Integer,Tuple3<Float,Float,Float>> graderAndscore){
            this.sname = sname;
            this.age = age;
            this.gender = gender;
            this.gradeAndscore = graderAndscore;
        }

        public String getSname() {
            return sname;
        }

        public void setSname(String sname) {
            this.sname = sname;
        }

        public String getGender() {
            return gender;
        }

        public void setGender(String gender) {
            this.gender = gender;
        }

        public int getAge() {
            return age;
        }

        public void setAge(int age) {
            this.age = age;
        }

        public Tuple2<Integer, Tuple3<Float, Float, Float>> getGradeAndscore() {
            return gradeAndscore;
        }

        public void setGradeAndscore(Tuple2<Integer, Tuple3<Float, Float, Float>> gradeAndscore) {
            this.gradeAndscore = gradeAndscore;
        }

        @Override
        public String toString() {
            return "StudentInfo{" +
                    "sname='" + sname + '\'' +
                    ", gender='" + gender + '\'' +
                    ", age=" + age +
                    ", gradeAndscore=" + gradeAndscore +
                    '}';
        }
    }
}


```

### 使用Key Selector Functions来指定key

```java
package com.szxy.virtualkey;

import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.operators.FlatMapOperator;
import org.apache.flink.api.java.operators.ReduceOperator;
import org.apache.flink.api.java.operators.UnsortedGrouping;

/**
 * @Auther:zwer
 * @Date:2019/12/26 15:05
 * @Description:com.szxy.virtualkey
 * @Version:1.0
 **/
public class FlinkKeySelectorVirutalKeyDemo {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataSource<String> dataSource = env.readTextFile("./data/studentInfo.txt");

        /**
         *  使用 KeySelector 指定虚拟 key
         */
        UnsortedGrouping<String> groupByGender = dataSource.groupBy(new KeySelector<String, String>() {
            @Override
            public String getKey(String s) throws Exception {
                return s.split(" ")[1];  // 按照拆分第二个字段
            }
        });

        ReduceOperator<String> result = groupByGender.reduce(new ReduceFunction<String>() {
            @Override
            public String reduce(String s, String t1) throws Exception {
                return s + "#" + t1;
            }
        });
        // 触发执行
        result.print();
    }
}

```

## Flink 与 Kafka整合

### Flink 读取 Kafka 中数据 jar 依赖

```xml
<!-- Flink Kafka连接器的依赖-->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.11_2.11</artifactId>
    <version>1.7.1</version>
</dependency>
```

### Flink 读取 Kafka 中数据

```
0.0.0.0 -127.0.0.1
128.0.0 -
```

